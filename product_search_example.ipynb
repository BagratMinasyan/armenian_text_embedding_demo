{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Query Similarity Search with Embeddings</h1>\n",
    "\n",
    "<p>This example demonstrates how to use embeddings to calculate the cosine similarity between queries and a sample dataset. We will use a small dataset containing categories like <strong>Smartphones</strong>, <strong>Audio Equipment</strong>, <strong>Kitchen Appliances</strong>, <strong>Athletic Footwear</strong>, and <strong>Home Cleaning</strong>. For each query, we embed the text, calculate the cosine similarity, and retrieve the most similar item.</p>\n",
    "\n",
    "<h2>Dataset</h2>\n",
    "<ul>\n",
    "    <li><strong>Smartphones</strong></li>\n",
    "    <li><strong>Audio Equipment</strong></li>\n",
    "    <li><strong>Kitchen Appliances</strong></li>\n",
    "    <li><strong>Athletic Footwear</strong></li>\n",
    "    <li><strong>Home Cleaning</strong></li>\n",
    "</ul>\n",
    "\n",
    "<h3>Example Process</h3>\n",
    "<p>For each category, we embedded the text descriptions, calculated cosine similarity, and identified the most similar item based on a query. Below are the queries and their corresponding most similar items:</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing Required Packages</h2>\n",
    "\n",
    "<p>Before starting, ensure you have all the necessary packages installed. If a package is missing, you can install it using <code>pip</code>. Below is the list of required imports for this project:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Dataset and Move Model to GPU (if available)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name Metric-AI/armenian-text-embeddings-1. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('Metric-AI/armenian-text-embeddings-1',device=device)\n",
    "dataset = load_from_disk('product_demo_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['name', 'description', 'item_section'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Passage Preprocessing Steps</h1>\n",
    "\n",
    "<p>In this example, product descriptions are treated as \"passages.\" Each passage is prefixed with <code>passage:</code>. The following preprocessing steps are applied:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752bd965db55447a9fcb70208d9f5c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad1a945b46d4a7c9b5536033bb39607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {'passage': 'passage: '+x['description']})\n",
    "dataset = dataset.map(lambda x: {'embedding': model.encode(x['passage'], normalize_embeddings=True)}, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Search for Similar Items in a Dataset Using Gradio</h2>\n",
    "\n",
    "<p>This interface allows you to input a query (e.g., a product or item description), and it will return the most similar item from a dataset based on the query.</p>\n",
    "\n",
    "<p><strong>Objective:</strong> Given a search query, the model will compute the similarity between the query and the embeddings of items in the dataset. It will then return the item with the highest similarity score.</p>\n",
    "\n",
    "<p><strong>Usage:</strong> You can either type a custom query in the input box or use one of the example queries provided. The app will search through the dataset and return the item with the highest similarity to the query.</p>\n",
    "\n",
    "<p>The app will output the <strong>Name</strong> and <strong>Item Section</strong> of the most similar item in the dataset.</p>\n",
    "\n",
    "<h3>Gradio Interface Features:</h3>\n",
    "<ul>\n",
    "    <li><strong>Input:</strong> A text box for entering the search query.</li>\n",
    "    <li><strong>Output:</strong> A text box showing the most similar item.</li>\n",
    "    <li><strong>Clear Button:</strong> A button to clear the input and output fields.</li>\n",
    "    <li><strong>Example Queries:</strong> Predefined queries that users can try.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "* Running on public URL: https://98035dcf937664984e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://98035dcf937664984e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\n",
    "    \"’¢’°÷Ä’±÷Ä’∏÷Ä’°’Ø ’Ω’¥’°÷Ä’©÷Ü’∏’∂’ù ’∞’¶’∏÷Ä ’∫÷Ä’∏÷Å’•’Ω’∏÷Ä’∏’æ\",\n",
    "    \"’°’≤’¥’∏÷Ç’Ø’® ’¥’•’Ø’∏÷Ç’Ω’°÷Å’∂’∏’≤ ’°’∂’¨’°÷Ä ’°’Ø’°’∂’ª’°’Ø’°’¨’∂’•÷Ä\",\n",
    "    \"’¢’°’¶’¥’°÷Ü’∏÷Ç’∂’Ø÷Å’´’∏’∂’°’¨ ’≠’∏’∞’°’∂’∏÷Å’°’µ’´’∂ ’£’∏÷Ä’Æ’´÷Ñ\",\n",
    "    \"’æ’°’¶÷Ñ’´ ’Ø’∏’∑’´’Ø’∂’•÷Ä\",\n",
    "    \"’º’∏’¢’∏’ø ÷É’∏’∑’•’Ø’∏÷Ç’¨\"\n",
    "]\n",
    "\n",
    "def search(query):\n",
    "    processed_query = 'query: ' + query\n",
    "    \n",
    "    query_embedding = model.encode(processed_query, normalize_embeddings=True)\n",
    "    \n",
    "    similarities = [np.dot(item['embedding'], query_embedding) for item in dataset]\n",
    "    \n",
    "    max_index = int(np.argmax(similarities))\n",
    "    \n",
    "    max_row = dataset[max_index]\n",
    "    \n",
    "    return f\"Name: {max_row['name']}\\n Item Section: {max_row['item_section']}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=search, \n",
    "    inputs=gr.Textbox(\n",
    "        label=\"üîç Enter Your Search Query\", \n",
    "        placeholder=\"Try one of the example queries...\",\n",
    "        value=queries[0]\n",
    "    ),\n",
    "    outputs=gr.Textbox(label=\"üèÜ Most Similar Item\"), \n",
    "    live=False,  \n",
    "    clear_btn=\"Clear\",\n",
    "    examples=[\n",
    "        [query] for query in queries \n",
    "    ]\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Query Similarity Calculation</h1>\n",
    "\n",
    "<p>The following steps are taken in the code to calculate the similarity between queries and product descriptions:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Query Processing:</strong> Each query is prefixed with <code>query:</code> to differentiate it from the product descriptions.</li>\n",
    "    <li><strong>Embedding Queries:</strong> The processed queries are embedded using a pre-trained model, and the embeddings are normalized.</li>\n",
    "    <li><strong>Similarity Calculation:</strong> For each query, the cosine similarity is calculated between the query embedding and the embeddings of the products in the dataset.</li>\n",
    "    <li><strong>Finding the Most Similar Product:</strong> The product with the highest similarity score is selected as the most similar item to the query. The name and description of the most similar product are printed for each query.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
